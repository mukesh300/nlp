{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\mukes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mukes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            string containing tweet\n",
    "        Output:\n",
    "            list of words containing processed tweet(tokenize, lowercase, remove punctuation/stopwords, stem the words)\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_en = stopwords.words('english')\n",
    "\n",
    "    tweet = re.sub(r'\\$\\w', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, preserve_case=False, reduce_len=True)\n",
    "\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "\n",
    "    for word in tweet_tokens:\n",
    "        if word not in stopwords_en and word not in string.punctuation:\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            tweets: list of tweets\n",
    "            ys: mx1 array of sentiment labels for each tweet\n",
    "        Output:\n",
    "            freqs: dictionary mapping each (word, sentiment) pair to it's frequency\n",
    "    \"\"\"\n",
    "    ys = ys.squeeze()\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair]+=1\n",
    "            else:\n",
    "                freqs[pair]=1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_tweets), len(neg_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos, test_pos = pos_tweets[:4000], pos_tweets[4000:]\n",
    "train_neg, test_neg = neg_tweets[:4000], neg_tweets[4000:]\n",
    "\n",
    "train_x = train_pos+train_neg\n",
    "test_x = test_pos+test_neg\n",
    "\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = build_freqs(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11659"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 3.119e+03, 1.160e+02]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
    "    \"\"\" \n",
    "        Input:\n",
    "            tweet: input tweet\n",
    "            freqs: dictionary mapping (word, label) to freq\n",
    "        Output:\n",
    "            x: feature vector of dimension (1, 3)\n",
    "    \"\"\"\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    x = np.zeros((1, 3))\n",
    "    x[0, 0] = 1\n",
    "\n",
    "    for word in word_l:\n",
    "        x[0, 1] += freqs.get((word, 1), 0)\n",
    "        x[0, 2] += freqs.get((word, 0), 0)\n",
    "\n",
    "    return x\n",
    "\n",
    "extract_features(train_x[2], freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :] = extract_features(train_x[i], freqs)\n",
    "\n",
    "Y = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data is 0.995\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "lr = LogisticRegression()\n",
    "lg_model = Pipeline([('standardize', scaler),\n",
    "                    ('log_reg', lr)])\n",
    "\n",
    "lg_model.fit(X, Y)\n",
    "\n",
    "pred_y_train = lg_model.predict(X)\n",
    "\n",
    "print(f\"accuracy score on train data is {accuracy_score(Y, pred_y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on test data is 0.994\n"
     ]
    }
   ],
   "source": [
    "X_test = np.zeros((len(test_x), 3))\n",
    "for i in range(len(test_x)):\n",
    "    X_test[i, :] = extract_features(test_x[i], freqs)\n",
    "\n",
    "Y_test = test_y\n",
    "\n",
    "pred_y_test = lg_model.predict(X_test)\n",
    "\n",
    "print(f\"accuracy score on test data is {accuracy_score(Y_test, pred_y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample data:  @Carouselballet Monday? :(\n",
      "processed sample:  ['monday', ':(']\n",
      "predicted result using Logistic regression:  negative\n"
     ]
    }
   ],
   "source": [
    "#sample test\n",
    "sample = test_x[1006]\n",
    "print(\"sample data: \", sample)\n",
    "print(\"processed sample: \", process_tweet(sample))\n",
    "result = \"positive\" if lg_model.predict(extract_features(sample, freqs))[0] == 1.0 else \"negative\"\n",
    "print(\"predicted result using Logistic regression: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freq_table(vocab_dict, freqs):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        vocab_dict: dictionary mapping index, word\n",
    "        freqs: dictionary mapping (word, label) to freq\n",
    "    Output:\n",
    "        freq_tbl: nx2 array containing pos and neg probabilities of words in vocab\n",
    "    \"\"\"\n",
    "    freq_tbl = np.zeros((len(vocab_dict), 2))\n",
    "    for i, word in vocab_dict.items():\n",
    "        freq_tbl[i, :] = [freqs.get((word, 0), 0), freqs.get((word, 1), 0)]\n",
    "\n",
    "    n_unique = np.count_nonzero(freq_tbl, axis=0)\n",
    "    n_count = np.sum(freq_tbl, axis=0)\n",
    "\n",
    "    # Laplacian smoothning\n",
    "    freq_tbl = np.divide(freq_tbl+1, n_unique+n_count)\n",
    "    return freq_tbl\n",
    "    \n",
    "def naive_bayes_predict(tweet, logprior, freq_tbl, vocab_dict_rev):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        tweet: input tweet\n",
    "        logprior: log of prior prob\n",
    "        freq_tbl: nx2 array containing pos and neg probabilities of words in vocab\n",
    "        vocab_dict_rev: dictionary mapping word, index\n",
    "    Output:\n",
    "        result: 1/0 indicating if tweet is pos/neg sentiment\n",
    "    \"\"\"\n",
    "    pred = logprior\n",
    "    for word in process_tweet(tweet):\n",
    "        if word in vocab_dict_rev.keys():\n",
    "            p_neg, p_pos = freq_tbl[vocab_dict_rev[word], :]\n",
    "            pred += (np.log(p_pos)-np.log(p_neg))\n",
    "    result = 1 if pred>0 else 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {i: word for i, word in enumerate(list(set([word for word, _ in freqs.keys()])))}\n",
    "vocab_dict_rev = {word: i for i, word in vocab_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data is 0.999375\n",
      "accuracy score on train data is 0.9955\n"
     ]
    }
   ],
   "source": [
    "freq_tbl = build_freq_table(vocab_dict, freqs)\n",
    "\n",
    "log_prior_train = float(np.log(sum(train_y)) - np.log(len(train_y)-sum(train_y)))\n",
    "pred_y_train = [naive_bayes_predict(tweet, log_prior_train, freq_tbl, vocab_dict_rev) for tweet in train_x]\n",
    "\n",
    "log_prior_test = float(np.log(sum(test_y)) - np.log(len(test_y)-sum(test_y)))\n",
    "pred_y_test = [naive_bayes_predict(tweet, log_prior_test, freq_tbl, vocab_dict_rev) for tweet in test_x]\n",
    "\n",
    "print(f\"accuracy score on train data is {accuracy_score(train_y, np.array(pred_y_train).reshape(len(pred_y_train), 1))}\")\n",
    "print(f\"accuracy score on train data is {accuracy_score(test_y, np.array(pred_y_test).reshape(len(pred_y_test), 1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb096cfdeb01fd1749b10f5bab37c08591e7b8a2cd3a8f8d54a9b4aa87469879"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
