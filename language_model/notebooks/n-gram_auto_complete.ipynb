{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mukes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/en_US.twitter.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(corpus):\n",
    "    \"\"\"\n",
    "    Tokenize sentences in corpus\n",
    "        Input:\n",
    "            corpus: large text\n",
    "        Output:\n",
    "            tokenized_sentences: 2D list of tokens \n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    sentences = corpus.split('\\n')\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = nltk.tokenize.word_tokenize(sentence)\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenize_sentence(data)\n",
    "random.seed(32)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    \"\"\"\n",
    "    Creates a word count dict out of tokenized sentences\n",
    "        Input:\n",
    "            tokenized_sentences: list of list of words\n",
    "        Output:\n",
    "            word_counts: word count dictionary\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            if word in word_counts:\n",
    "                word_counts[word]+=1\n",
    "            else:\n",
    "                word_counts[word]=1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = count_words(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([929., 175.,  83.,  53.,  30.,  24.,  17.,  18.,   6.,  12.]),\n",
       " array([ 2000.,  6800., 11600., 16400., 21200., 26000., 30800., 35600.,\n",
       "        40400., 45200., 50000.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(word_counts.values(), range=(2000, 50000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(505137, 32353)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts), sum([1 for val in word_counts.values() if val>20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closed_vocab(tokenized_data, threshold):\n",
    "    \"\"\"\n",
    "    Function to create list of words with freq greater than threshold\n",
    "        Input:\n",
    "            tokenized_data: list of tokenized sentences\n",
    "            threshold: int value used as cut off\n",
    "        Output:\n",
    "            closed_vocab: list of words with freq greater than threshold\n",
    "    \"\"\"\n",
    "    closed_vocab = []\n",
    "    word_counts = count_words(tokenized_data)\n",
    "    for word, count in word_counts.items():\n",
    "        if count>=threshold:\n",
    "            closed_vocab.append(word)\n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov(tokenized_sentences, vocab, unk_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Function to replace out of vocabulary words from the tokenized sentences\n",
    "        Input:\n",
    "            tokenized_sentences: list of tokenized sentences\n",
    "            vocab: list of closed vocab of words\n",
    "            unk_token: token used to replace oov words\n",
    "        Output:\n",
    "            replaced_tokenized_sentences: updated tokenized_sentences\n",
    "    \"\"\"\n",
    "    vocabulary = set(vocab)\n",
    "    replaced_tokenized_sentences = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in vocabulary:\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                replaced_sentence.append(unk_token)\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "\n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold):\n",
    "    \"\"\"\n",
    "    Helper function to process tokenized data from closed vocabulary\n",
    "        Input:\n",
    "            train_data: tokenized train data\n",
    "            test_data: tokenized test data\n",
    "            count_threshold: word count used to generate closed vocab\n",
    "        Output:\n",
    "            train_data_replaced: processed train data\n",
    "            test_data_replaced: processed test data\n",
    "            vocabulary: closed vocab of words\n",
    "    \"\"\"\n",
    "    vocabulary = get_closed_vocab(tokenized_data, count_threshold)\n",
    "    train_data_replaced = replace_oov(train_data, vocabulary)\n",
    "    test_data_replaced = replace_oov(test_data, vocabulary)\n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data, minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token='<e>'):\n",
    "    \"\"\"\n",
    "    Generate n-gram freq dict\n",
    "        Input:\n",
    "            data: list of tokenized seq of words \n",
    "            n: value of n to calculate n-grams\n",
    "            start_token: token to append at the start of sentence\n",
    "            end_token: token to append at the end of sentence\n",
    "        Output:\n",
    "            n_grams: n_gram dict\n",
    "            \n",
    "    \"\"\"\n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = [start_token]*n+sentence+[end_token]\n",
    "\n",
    "        sentence = tuple(sentence)\n",
    "\n",
    "        m = len(sentence) if n==1 else len(sentence)-1\n",
    "        for i in range(m):\n",
    "            n_gram = sentence[i:i+n]\n",
    "\n",
    "            if n_gram in n_grams:\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_prob(word, prev_n_gram, n_gram_counts, n_plus1_gram_counts, vocab_size, k=1):\n",
    "    \"\"\"\n",
    "    Generate prob of next word based on n and n+1 gram counts\n",
    "        Input:\n",
    "            word: next word prob to calculate\n",
    "            prev_n_gram: tuple of words of length n\n",
    "            n_gram_counts: dict of n_grams and it freq\n",
    "            n_plus1_gram_counts: dict of nplus1_grams and it freq\n",
    "            vocab_size: len of vocabulary\n",
    "            k: constant for smoothning\n",
    "        Output:\n",
    "            prob: next word probability\n",
    "            \n",
    "    \"\"\"\n",
    "    prev_n_gram = tuple(prev_n_gram)\n",
    "\n",
    "    prev_n_gram_count = n_gram_counts.get(prev_n_gram, 0)\n",
    "    denominator = prev_n_gram_count + vocab_size*k\n",
    "\n",
    "    n_plus1_gram = prev_n_gram+(word,)\n",
    "\n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
    "    numerator = n_plus1_gram_count+k\n",
    "\n",
    "    prob = numerator/denominator\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_prob_vocab(prev_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1):\n",
    "    \"\"\"\n",
    "    Generate prob of words from vocabulary based on n and n+1 gram counts\n",
    "        Input:\n",
    "            prev_n_gram: tuple of words of length n\n",
    "            n_gram_counts: dict of n_grams and it freq\n",
    "            n_plus1_gram_counts: dict of nplus1_grams and it freq\n",
    "            vocabulary: closed vocab list of words\n",
    "            k: constant for smoothning\n",
    "        Output:\n",
    "            probabilities: dict of vocabulary and their word probs\n",
    "            \n",
    "    \"\"\"\n",
    "    prev_n_gram = tuple(prev_n_gram)\n",
    "    vocabulary = vocabulary+['<s>', '<e>']\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        prob = estimate_prob(word, prev_n_gram, n_gram_counts, n_plus1_gram_counts, vocab_size, k=k)\n",
    "\n",
    "        probabilities[word] = prob\n",
    "    return probabilities    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(n_gram_counts, n_plus1_gram_counts, vocabulary, init_word='<s>', max_len=10, k=1, estimate_prob_vocab=estimate_prob_vocab):\n",
    "    \"\"\"\n",
    "    Helper fuction to generate sequence of words\n",
    "        Input:\n",
    "            n_gram_counts: dict of n_grams and it freq\n",
    "            n_plus1_gram_counts: dict of nplus1_grams and it freq\n",
    "            vocabulary: closed vocab list of words\n",
    "            init_word: any random word to initialize the seq\n",
    "            max_len: max len of the seq generated\n",
    "            k: constant for smoothning\n",
    "            estimate_prob_vocab: function to generate probs of the next words from vocab\n",
    "        Output:\n",
    "            gen_sentence: generated sequence of words\n",
    "            \n",
    "    \"\"\"\n",
    "    n = len(list(n_gram_counts.keys())[0])-1\n",
    "    prev_word = ['<s>']*n+[init_word]\n",
    "    prev_n_gram = tuple(prev_word)\n",
    "\n",
    "    i=0\n",
    "    gen_sentence = [init_word]\n",
    "\n",
    "    while i < max_len:\n",
    "        probabilities = estimate_prob_vocab(prev_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k)\n",
    "\n",
    "        sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "        word = sorted_probs[0][0]\n",
    "\n",
    "        if word == prev_n_gram[-1]:\n",
    "            word = sorted_probs[1][0]\n",
    "\n",
    "        if word == '<e>':\n",
    "            break\n",
    "\n",
    "        gen_sentence = gen_sentence+[word]\n",
    "        prev_n_gram = prev_n_gram[1:]+(word,)\n",
    "\n",
    "        i+=1\n",
    "    print(' '.join(gen_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where are you ?\n"
     ]
    }
   ],
   "source": [
    "bi_gram = count_n_grams(train_data_processed, 2)\n",
    "tri_gram = count_n_grams(train_data_processed, 3)\n",
    "\n",
    "generate_seq(bi_gram, tri_gram, vocabulary, init_word='where')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many times i 've been waiting for the follow !\n"
     ]
    }
   ],
   "source": [
    "five_gram = count_n_grams(train_data_processed, 2)\n",
    "six_gram = count_n_grams(train_data_processed, 3)\n",
    "\n",
    "generate_seq(five_gram, six_gram, vocabulary, init_word='how')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb096cfdeb01fd1749b10f5bab37c08591e7b8a2cd3a8f8d54a9b4aa87469879"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
